{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSGAA_SNG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "17ZkrhMDPOMEnnGzpJ5UKwcaNzyvpV18k",
      "authorship_tag": "ABX9TyOJJyOJmzYL9Zvz9T+THqjJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuraged51a/LSGAA_SNG/blob/main/LSGAA_SNG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMWu1DJNjtjO"
      },
      "source": [
        "## Data Preprocessing\r\n",
        "We have loaded the events of the 26th March 2020 for reference, from the GDELT website. The [dataset](https://drive.google.com/file/d/1BjG_HA-TXWxU0xDxIampqa2gP6fs574z/view?usp=sharing) contains more than 1.5 lakh rows, which is very difficult to process.<br><br>\r\n",
        "We aim to make use of news events coming only from some of the major news websites which are listed below, and only going to use the news which has some reference to the U.K. After filtering through the given websites we still attain a massive dataset of 708 rows.\r\n",
        "\r\n",
        "\r\n",
        "*   www.dailymail.co.uk\r\n",
        "*   www.express.co.uk\r\n",
        "*   www.independent.co.uk\r\n",
        "*   www.mirror.co.uk\r\n",
        "*   www.standard.co.uk\r\n",
        "*   www.bbc.co.uk\r\n",
        "*   www.thetimes.co.uk\r\n",
        "*   www.dailystar.co.uk\r\n",
        "*   www.hulldailymail.co.uk\r\n",
        "*   www.eveningexpress.co.uk\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_lcAQXtjyDx",
        "outputId": "66cf9af9-52f0-4ce0-d789-eb4b89348775"
      },
      "source": [
        "# Installing Libraries\r\n",
        "! pip install goose3\r\n",
        "! pip install flair"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: goose3 in /usr/local/lib/python3.7/dist-packages (3.1.8)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from goose3) (0.42.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from goose3) (3.2.5)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.7/dist-packages (from goose3) (1.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from goose3) (4.6.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from goose3) (2.8.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from goose3) (7.0.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from goose3) (4.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from goose3) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->goose3) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->goose3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->goose3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->goose3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->goose3) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC5zoPz0lijP"
      },
      "source": [
        "# Importing Libraries\r\n",
        "import pandas as pd\r\n",
        "import string\r\n",
        "import re\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import torch\r\n",
        "import nltk\r\n",
        "\r\n",
        "from goose3 import Goose\r\n",
        "from itertools import combinations, product\r\n",
        "form tqdm import tqdm\r\n",
        "from flair.data import Sentence\r\n",
        "from flair.models import SequenceTagger\r\n",
        "from nltk import tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDkSVBCJizhi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c90b1bc-6bab-4da3-e4a7-28bf1ecf7adb"
      },
      "source": [
        "# Load Data\r\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/LSGAA_SNG/url_dataset.csv')\r\n",
        "len(df1['URL'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "708"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOLBq0Z8ikSR"
      },
      "source": [
        "## Extracting the Raw Text from URLs\r\n",
        "We will be using [goose3](https://goose3.readthedocs.io/en/latest/index.html) to extract the raw text from these news URLs shortlisted in the previous step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htuDtLOqjj3q"
      },
      "source": [
        "g = Goose()\r\n",
        "dateList, domainList, titleList, contentList = [], [], [], []\r\n",
        "for item in df1['URL']:\r\n",
        "  try:\r\n",
        "    article = g.extract(url = str(item))\r\n",
        "    dateList.append('2020-03-26')\r\n",
        "    domainList.append(article.domain)\r\n",
        "    titleList.append(article.title)\r\n",
        "    contentList.append(article.cleaned_text)\r\n",
        "  except Exception as e:\r\n",
        "    print(str(item))\r\n",
        "    continue\r\n",
        "g.close()\r\n",
        "print(\"Extraction Completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0dzlffUAI4C"
      },
      "source": [
        "# Combining these fields to form a new DataFrame.\r\n",
        "dic = {'date': dateList, 'domain': domainList, 'title': titleList, 'content': contentList}\r\n",
        "df2 = pd.DataFrame(data = dic)\r\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF3OzfCqHcwW"
      },
      "source": [
        "## Entity Recognition\r\n",
        "Named Entity Recognition or Entity Extraction is a subtask of Information Extraction that seeks to locate and classify named entitities mentioned in the unstructured text to pre-defined  categories. We can use multitude of libraries for this purpose like GATE, OpenNLP, SpaCy etc. For this project, we aim to utilise the [Flair](https://github.com/flairNLP/flair) library to identify the individuals and orgainsations of interest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfq_1sZcHcB_"
      },
      "source": [
        "# Downloading NLTK's punkt tokenizer\r\n",
        "nltk.download('punkt')\r\n",
        "# Loading Flair's ER model\r\n",
        "tagger = SequenceTagger.load('ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WuuEs52MMr-"
      },
      "source": [
        "#Removing Pronouns\r\n",
        "pronouns = ['I', 'You', 'It', 'He', 'She', 'We', 'They']\r\n",
        "suffixes = [\"\", \"'m\", \"'re\", \"'s\", \"'ve\", \"'d\", ]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}